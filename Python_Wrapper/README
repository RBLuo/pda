This repository contains the source codes used in the manuscript "Revisiting Probability Distribution Assumptions for Information Theoretic Feature Selection".

The source codes are written in Python and C++. The software and packages used are Python-3.5.2, Boost.Python-1.66.0, GCC-6.2.0, and SKlearn. 

To compile the C++ codes, simply go to the "fsmethods" folder and run the MakeFile in terminal. We have successfully compiled the C++ codes in a Linux and MaxOS environment. However we haven't test it on a Windows environment. The compiled codes will be used by the Python main file named "main.py" to perform feature selection and classification. The input to run the main file is "python3 main.py FSMethod Dataset". 

The feature selection methods that you can select from are VMIrm (\tilde{J}_{AMD}^{1}), VMIgm (\tilde{J}_{GMD}^{1}), VMIin (\tilde{J}_{FID}^{1}), RMRMRrm (J_{AMD}^{2,1}), JMIrm (J_{AMD}^{1,1}), MRMRrm (J_{AMD}^{1,0}), RelaxMRMR (J_{GMD}^{2,1}), JMI (J_{GMD}^{1,1}), MRMR (J_{GMD}^{1,0}), CIFE (J_{FID}^{1,1}), MIFS (J_{FID}^{1,0}), and MIM. 

We have included a couple of datasets in the "datasets" folder so that you can have a try, e.g., wine, congress, heart, etc. 

As a concrete example, typing in the terminal "python main.py VMIrm wine" will run the \tilde{J}_{AMD}^{1} method on the wine dataset and the output will be the selected features, running time and classification accuracy, which can be found in the "results" folder.

The source code might be hard to follow, as we have not got much time to clean up it. Sorry for the messy; but we will improve the readability of codes later.
